Task 2 was solved using the Monte Carlo Dropout approach as explained in the task description. The width and the depth of the predefined MNISTNet was increased (an additional layer and all layers set to 200 Neurons) to achieve a better score. Increasing the depth and the width of the Net too much resulted in overfitting and worse results. The dropout probability was set to 0.6 and the batch_size was increased. The Network was trained with the Adam optimizer and the same loss function as defined in Dummy Network (log softwax and negatie log likelihood loss). The predicted probabilities were produced by predicting 100 samples and then averaging their estimated probability.